{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-19 16:52:49.279639: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-19 16:52:49.279683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-19 16:52:49.280695: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-19 16:52:49.286269: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-19 16:52:49.899764: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Comparison model lab\n",
    "from keras import layers, regularizers, models, backend, initializers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def create_comparisonModel(shared_base, l=0.001):\n",
    "    input_shape = shared_base.input_shape[1:]\n",
    "    \n",
    "    input_a = layers.Input(shape=input_shape, dtype='float32', name='input_a')\n",
    "    input_b = layers.Input(shape=input_shape, dtype='float32', name='input_b')\n",
    "\n",
    "    output_a = shared_base(input_a)\n",
    "    output_b = shared_base(input_b)\n",
    "\n",
    "    flat_a = layers.Flatten()(output_a)\n",
    "    flat_b = layers.Flatten()(output_b)\n",
    "\n",
    "\n",
    "    # fc6_a = layers.Dense(1024, activation='relu', name='Comparisonfc6_a', kernel_regularizer=regularizers.l2(l))(flat_a)\n",
    "    # drop6_a = layers.Dropout(0.5, name='ComparisonDropout6_a')(fc6_a)\n",
    "    # fc7_a = layers.Dense(512, activation='relu', name='Comparisonfc7_a', kernel_regularizer=regularizers.l2(l))(drop6_a)\n",
    "    # drop7_a = layers.Dropout(0.5, name='ComparisonDropout7_a')(fc7_a)\n",
    "    # # embedding_a = layers.Dense(256, activation=None, name='embedding_a')(drop7_a)\n",
    "\n",
    "    # fc6_b = layers.Dense(1024, activation='relu', name='Comparisonfc6_b', kernel_regularizer=regularizers.l2(l))(flat_b)\n",
    "    # drop6_b = layers.Dropout(0.5, name='ComparisonDropout6_b')(fc6_b)\n",
    "    # fc7_b = layers.Dense(512, activation='relu', name='Comparisonfc7_b', kernel_regularizer=regularizers.l2(l))(drop6_b)\n",
    "    # drop7_b = layers.Dropout(0.5, name='ComparisonDropout7_b')(fc7_b)\n",
    "    # # embedding_b = layers.Dense(256, activation=None, name='embedding_b')(drop7_b)    \n",
    "\n",
    "    fc6_a = layers.Dense(1024, activation='relu', name='Comparisonfc6_a', kernel_regularizer=regularizers.l2(l),\n",
    "                          kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(flat_a)\n",
    "    drop6_a = layers.Dropout(0.5, name='ComparisonDropout6_a')(fc6_a)\n",
    "    fc7_a = layers.Dense(512, activation='relu', name='Comparisonfc7_a', kernel_regularizer=regularizers.l2(l),\n",
    "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(drop6_a)\n",
    "    drop7_a = layers.Dropout(0.5, name='ComparisonDropout7_a')(fc7_a)\n",
    "    fc8_a = layers.Dense(256, activation='relu', name='Comparisonfc8_a',  kernel_regularizer=regularizers.l2(l),\n",
    "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(drop7_a)\n",
    "    drop8_a = layers.Dropout(0.5, name='ComparisonDropout8_a')(fc8_a)\n",
    "    fc9_a = layers.Dense(1, name='Comparisonfc9_a',  kernel_regularizer=regularizers.l2(l),\n",
    "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(drop8_a)\n",
    "    \n",
    "    \n",
    "    fc6_b = layers.Dense(1024, activation='relu', name='Comparisonfc6_b', kernel_regularizer=regularizers.l2(l),\n",
    "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(flat_b)\n",
    "    drop6_b = layers.Dropout(0.5, name='ComparisonDropout6_b')(fc6_b)\n",
    "    fc7_b = layers.Dense(512, activation='relu', name='Comparisonfc7_b', kernel_regularizer=regularizers.l2(l), \n",
    "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(drop6_b)\n",
    "    drop7_b = layers.Dropout(0.5, name='ComparisonDropout7_b')(fc7_b)\n",
    "    fc8_b = layers.Dense(256, activation='relu', name='Comparisonfc8_b',  kernel_regularizer=regularizers.l2(l), \n",
    "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(drop7_b)\n",
    "    drop8_b = layers.Dropout(0.5, name='ComparisonDropout8_b')(fc8_b)\n",
    "    fc9_b = layers.Dense(1, name='Comparisonfc9_b',  kernel_regularizer=regularizers.l2(l),\n",
    "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(drop8_b)\n",
    "\n",
    "    difference = layers.Subtract(name='Difference')([fc9_a, fc9_b])\n",
    "\n",
    "    comparison_output = layers.Activation('tanh', name='comparison_output')(difference)\n",
    "\n",
    "    # # Use L1 distance for comparison\n",
    "    # difference = layers.Subtract(name='Difference')([drop7_a, drop7_b])\n",
    "\n",
    "    # # Sum the elements of the difference vector to get a scalar\n",
    "    # summed_difference = layers.Lambda(lambda tensors: tf.reduce_sum(tensors, axis=-1, keepdims=True), name='Summed_Difference')(difference)\n",
    "\n",
    "    # comparison_output = layers.Dense(1, activation='tanh', name='comparison_output')(summed_difference)\n",
    "\n",
    "    # # # concatenated = layers.Concatenate()([flat_a, flat_b])\n",
    "\n",
    "\n",
    "\n",
    "    comparison_model = models.Model(inputs=[input_a, input_b], outputs=comparison_output, name='comparison_model')\n",
    "\n",
    "\n",
    "    return input_a, input_b, comparison_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = np.random.random((8960,1))\n",
    "fb = np.random.random((8960,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of difference: (None, 1, 1)\n",
      "Input1 shape: (None, 8960, 1)\n",
      "Input2 shape: (None, 8960, 1)\n",
      "Output shape: (None, 1, 1)\n",
      "Shape of difference: (None, 1, 1)\n",
      "Difference values: [[[0.0181072578]]]\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "Output value: [[[-0.03055112]]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from keras import layers, models, regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "class Base(models.Model):\n",
    "    def __init__(self, l=0.001):\n",
    "        super(Base, self).__init__()\n",
    "        self.l = l\n",
    "        \n",
    "        self.model = models.Sequential([\n",
    "            layers.Input(shape=(8960, 1)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(l),\n",
    "                          kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(l),\n",
    "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(1,kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros'),\n",
    "            layers.Reshape((1, 1))\n",
    "        ])\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    # fc8_a = layers.Dense(256, activation='relu', name='Comparisonfc8_a',  kernel_regularizer=regularizers.l2(l),\n",
    "    #                      kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(drop7_a)\n",
    "    # drop8_a = layers.Dropout(0.5, name='ComparisonDropout8_a')(fc8_a)\n",
    "    # fc9_a = layers.Dense(1, name='Comparisonfc9_a',  kernel_regularizer=regularizers.l2(l),\n",
    "    #                      kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(drop8_a)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.model(inputs)\n",
    "\n",
    "class SuperModelComparison(models.Model):\n",
    "    def __init__(self, l=0.001):\n",
    "        super(SuperModelComparison, self).__init__()\n",
    "        self.base = base\n",
    "        self.dense = layers.Dense(1, activation='tanh', name='comparison_output')\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Assuming inputs is a tuple of two tensors\n",
    "        input1, input2 = inputs\n",
    "        \n",
    "        # Pass inputs through Base models\n",
    "        output_a1 = self.base(input1)\n",
    "        output_a2 = self.base(input2)\n",
    "        \n",
    "        tf.print(\"output_a1 values:\", output_a1)\n",
    "        tf.print(\"output_a2 values:\", output_a2)\n",
    "\n",
    "        # Element-wise subtraction\n",
    "        difference = layers.Subtract(name='Difference')([output_a1, output_a2])\n",
    "        tf.print(\"Difference values:\", difference)\n",
    "\n",
    "        print(f\"Shape of difference: {difference.shape}\")\n",
    "\n",
    "        # Sum the elements of the difference vector to get a scalar\n",
    "        # summed_difference = layers.Lambda(lambda tensors: tf.reduce_sum(tensors, axis=1, keepdims=True), name='Summed_Difference')(difference)\n",
    "\n",
    "        # Final comparison output\n",
    "        comparison_output = self.dense(difference)\n",
    "        \n",
    "        return comparison_output\n",
    "\n",
    "# Create the complete model\n",
    "input1 = layers.Input(shape=(8960, 1))\n",
    "input2 = layers.Input(shape=(8960, 1))\n",
    "base = Base()\n",
    "# base_output1 = base(input1)\n",
    "# base_output2 = base(input2)\n",
    "\n",
    "super_model = SuperModelComparison(base)\n",
    "output = super_model([input1, input2])\n",
    "\n",
    "\n",
    "print(f\"Input1 shape: {input1.shape}\")\n",
    "print(f\"Input2 shape: {input2.shape}\")\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Create sample inputs\n",
    "sample_input_1 = tf.random.normal((1, 8960, 1))\n",
    "sample_input_2 = tf.random.normal((1, 8960, 1))\n",
    "\n",
    "\n",
    "# Perform a forward pass using the predict method\n",
    "actual_output = super_model.predict([sample_input_1, sample_input_2])\n",
    "print(f\"Output value: {actual_output}\")\n",
    "\n",
    "\n",
    "# # # Create the final model\n",
    "# final_model = models.Model(inputs=[input1, input2], outputs=output)\n",
    "# actual_output = final_model.predict([input1, input2])\n",
    "\n",
    "# final_model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lastenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
