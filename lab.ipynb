{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 22:55:22.284097: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-26 22:55:22.284136: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-26 22:55:22.285218: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-26 22:55:22.291070: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-26 22:55:23.408033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Comparison model lab\n",
    "from keras import layers, regularizers, models, backend, initializers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def create_comparisonModel(shared_base, l=0.001):\n",
    "    input_shape = shared_base.input_shape[1:]\n",
    "    \n",
    "    input_a = layers.Input(shape=input_shape, dtype='float32', name='input_a')\n",
    "    input_b = layers.Input(shape=input_shape, dtype='float32', name='input_b')\n",
    "\n",
    "    output_a = shared_base(input_a)\n",
    "    output_b = shared_base(input_b)\n",
    "\n",
    "    flat_a = layers.Flatten()(output_a)\n",
    "    flat_b = layers.Flatten()(output_b)\n",
    "\n",
    "\n",
    "    # fc6_a = layers.Dense(1024, activation='relu', name='Comparisonfc6_a', kernel_regularizer=regularizers.l2(l))(flat_a)\n",
    "    # drop6_a = layers.Dropout(0.5, name='ComparisonDropout6_a')(fc6_a)\n",
    "    # fc7_a = layers.Dense(512, activation='relu', name='Comparisonfc7_a', kernel_regularizer=regularizers.l2(l))(drop6_a)\n",
    "    # drop7_a = layers.Dropout(0.5, name='ComparisonDropout7_a')(fc7_a)\n",
    "    # # embedding_a = layers.Dense(256, activation=None, name='embedding_a')(drop7_a)\n",
    "\n",
    "    # fc6_b = layers.Dense(1024, activation='relu', name='Comparisonfc6_b', kernel_regularizer=regularizers.l2(l))(flat_b)\n",
    "    # drop6_b = layers.Dropout(0.5, name='ComparisonDropout6_b')(fc6_b)\n",
    "    # fc7_b = layers.Dense(512, activation='relu', name='Comparisonfc7_b', kernel_regularizer=regularizers.l2(l))(drop6_b)\n",
    "    # drop7_b = layers.Dropout(0.5, name='ComparisonDropout7_b')(fc7_b)\n",
    "    # # embedding_b = layers.Dense(256, activation=None, name='embedding_b')(drop7_b)    \n",
    "\n",
    "    fc6_a = layers.Dense(1024, activation='relu', name='Comparisonfc6_a', kernel_regularizer=regularizers.l2(l),\n",
    "                          kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(flat_a)\n",
    "    drop6_a = layers.Dropout(0.5, name='ComparisonDropout6_a')(fc6_a)\n",
    "    fc7_a = layers.Dense(512, activation='relu', name='Comparisonfc7_a', kernel_regularizer=regularizers.l2(l),\n",
    "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(drop6_a)\n",
    "    drop7_a = layers.Dropout(0.5, name='ComparisonDropout7_a')(fc7_a)\n",
    "    fc8_a = layers.Dense(256, activation='relu', name='Comparisonfc8_a',  kernel_regularizer=regularizers.l2(l),\n",
    "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(drop7_a)\n",
    "    drop8_a = layers.Dropout(0.5, name='ComparisonDropout8_a')(fc8_a)\n",
    "    fc9_a = layers.Dense(1, name='Comparisonfc9_a',  kernel_regularizer=regularizers.l2(l),\n",
    "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(drop8_a)\n",
    "    \n",
    "    \n",
    "    fc6_b = layers.Dense(1024, activation='relu', name='Comparisonfc6_b', kernel_regularizer=regularizers.l2(l),\n",
    "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(flat_b)\n",
    "    drop6_b = layers.Dropout(0.5, name='ComparisonDropout6_b')(fc6_b)\n",
    "    fc7_b = layers.Dense(512, activation='relu', name='Comparisonfc7_b', kernel_regularizer=regularizers.l2(l), \n",
    "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(drop6_b)\n",
    "    drop7_b = layers.Dropout(0.5, name='ComparisonDropout7_b')(fc7_b)\n",
    "    fc8_b = layers.Dense(256, activation='relu', name='Comparisonfc8_b',  kernel_regularizer=regularizers.l2(l), \n",
    "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(drop7_b)\n",
    "    drop8_b = layers.Dropout(0.5, name='ComparisonDropout8_b')(fc8_b)\n",
    "    fc9_b = layers.Dense(1, name='Comparisonfc9_b',  kernel_regularizer=regularizers.l2(l),\n",
    "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(drop8_b)\n",
    "\n",
    "    difference = layers.Subtract(name='Difference')([fc9_a, fc9_b])\n",
    "\n",
    "    comparison_output = layers.Activation('tanh', name='comparison_output')(difference)\n",
    "\n",
    "    # # Use L1 distance for comparison\n",
    "    # difference = layers.Subtract(name='Difference')([drop7_a, drop7_b])\n",
    "\n",
    "    # # Sum the elements of the difference vector to get a scalar\n",
    "    # summed_difference = layers.Lambda(lambda tensors: tf.reduce_sum(tensors, axis=-1, keepdims=True), name='Summed_Difference')(difference)\n",
    "\n",
    "    # comparison_output = layers.Dense(1, activation='tanh', name='comparison_output')(summed_difference)\n",
    "\n",
    "    # # # concatenated = layers.Concatenate()([flat_a, flat_b])\n",
    "\n",
    "\n",
    "\n",
    "    comparison_model = models.Model(inputs=[input_a, input_b], outputs=comparison_output, name='comparison_model')\n",
    "\n",
    "\n",
    "    return input_a, input_b, comparison_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = np.random.random((8960,1))\n",
    "fb = np.random.random((8960,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 22:55:24.674852: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:55:24.709682: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:55:24.709856: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:55:24.710835: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:55:24.710975: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:55:24.711096: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:55:24.771855: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:55:24.772059: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:55:24.772193: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-26 22:55:24.772289: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5308 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of difference: (None, 1, 1)\n",
      "Input1 shape: (None, 8960, 1)\n",
      "Input2 shape: (None, 8960, 1)\n",
      "Output shape: (None, 1, 1)\n",
      "Shape of difference: (None, 1, 1)\n",
      "output_a1 values: [[[0.0270704068]]]\n",
      "output_a2 values: [[[-0.0195018798]]]\n",
      "Difference values: [[[0.0465722866]]]\n",
      "1/1 [==============================] - 0s 208ms/step\n",
      "Output value: [[[-0.05536409]]]\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models, regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "class Base(models.Model):\n",
    "    def __init__(self, l=0.001):\n",
    "        super(Base, self).__init__()\n",
    "        self.l = l\n",
    "        \n",
    "        self.model = models.Sequential([\n",
    "            layers.Input(shape=(8960, 1)),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(l),\n",
    "                          kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(l),\n",
    "                         kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros'),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(1,kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros'),\n",
    "            layers.Reshape((1, 1))\n",
    "        ])\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "    # fc8_a = layers.Dense(256, activation='relu', name='Comparisonfc8_a',  kernel_regularizer=regularizers.l2(l),\n",
    "    #                      kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(drop7_a)\n",
    "    # drop8_a = layers.Dropout(0.5, name='ComparisonDropout8_a')(fc8_a)\n",
    "    # fc9_a = layers.Dense(1, name='Comparisonfc9_a',  kernel_regularizer=regularizers.l2(l),\n",
    "    #                      kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')(drop8_a)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.model(inputs)\n",
    "\n",
    "class SuperModelComparison(models.Model):\n",
    "    def __init__(self, l=0.001):\n",
    "        super(SuperModelComparison, self).__init__()\n",
    "        self.base = base\n",
    "        self.dense = layers.Dense(1, activation='tanh', name='comparison_output')\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Assuming inputs is a tuple of two tensors\n",
    "        input1, input2 = inputs\n",
    "        \n",
    "        # Pass inputs through Base models\n",
    "        output_a1 = self.base(input1)\n",
    "        output_a2 = self.base(input2)\n",
    "        \n",
    "        tf.print(\"output_a1 values:\", output_a1)\n",
    "        tf.print(\"output_a2 values:\", output_a2)\n",
    "\n",
    "        # Element-wise subtraction\n",
    "        difference = layers.Subtract(name='Difference')([output_a1, output_a2])\n",
    "        tf.print(\"Difference values:\", difference)\n",
    "\n",
    "        print(f\"Shape of difference: {difference.shape}\")\n",
    "\n",
    "        # Sum the elements of the difference vector to get a scalar\n",
    "        # summed_difference = layers.Lambda(lambda tensors: tf.reduce_sum(tensors, axis=1, keepdims=True), name='Summed_Difference')(difference)\n",
    "\n",
    "        # Final comparison output\n",
    "        comparison_output = self.dense(difference)\n",
    "        \n",
    "        return comparison_output\n",
    "\n",
    "# Create the complete model\n",
    "input1 = layers.Input(shape=(8960, 1))\n",
    "input2 = layers.Input(shape=(8960, 1))\n",
    "base = Base()\n",
    "# base_output1 = base(input1)\n",
    "# base_output2 = base(input2)\n",
    "\n",
    "super_model = SuperModelComparison(base)\n",
    "output = super_model([input1, input2])\n",
    "\n",
    "\n",
    "print(f\"Input1 shape: {input1.shape}\")\n",
    "print(f\"Input2 shape: {input2.shape}\")\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Create sample inputs\n",
    "sample_input_1 = tf.random.normal((1, 8960, 1))\n",
    "sample_input_2 = tf.random.normal((1, 8960, 1))\n",
    "\n",
    "\n",
    "# Perform a forward pass using the predict method\n",
    "actual_output = super_model.predict([sample_input_1, sample_input_2])\n",
    "print(f\"Output value: {actual_output}\")\n",
    "\n",
    "\n",
    "# # # Create the final model\n",
    "# final_model = models.Model(inputs=[input1, input2], outputs=output)\n",
    "# actual_output = final_model.predict([input1, input2])\n",
    "\n",
    "# final_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1005126830.py, line 468)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 468\u001b[0;36m\u001b[0m\n\u001b[0;31m    return r, p, lo, hi    N = len(x)\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy import stats\n",
    "from numpy.polynomial.polynomial import polyfit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from keras.utils import get_custom_objects\n",
    "from shared_feature_extractor import create_shared_feature_extractor\n",
    "from rating_model import create_ratingModel\n",
    "from comparison_model import create_comparisonModel\n",
    "from keras import models, layers, regularizers, initializers\n",
    "from keras.layers import Layer\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "data_folder = '../../Calista/website-aesthetics-datasets-master/rating-based-dataset/preprocess/'\n",
    "\n",
    "train_data_path = data_folder + 'train_means_list.csv'\n",
    "test_data_path = data_folder + 'test_list.csv'\n",
    "images_path = data_folder + 'resized'\n",
    "image_map_folder = 'image_map/'\n",
    "train_pairs_csv = image_map_folder + 'train_image_pairs.csv'\n",
    "test_pairs_csv = image_map_folder + 'test_image_pairs.csv'\n",
    "\n",
    "def get_scores(scores_path):\n",
    "    images = []\n",
    "    scores = []\n",
    "    with open(scores_path) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                line_count += 1\n",
    "            else:\n",
    "                scores.append(float(row[1]))\n",
    "                line_count += 1\n",
    "                image_name = row[0]\n",
    "                images.append(images_path + '/' + image_name)\n",
    "    return (images, scores)\n",
    "\n",
    "def get_image_pairs_and_labels(pairs_csv_path):\n",
    "    image_pairs = []\n",
    "    labels = []\n",
    "    with open(pairs_csv_path) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        next(csv_reader)  # Skip header\n",
    "        for row in csv_reader:\n",
    "            image_path1 = row[0]\n",
    "            image_path2 = row[1]\n",
    "            label = int(row[2])\n",
    "            image_pairs.append((image_path1, image_path2))\n",
    "            labels.append(label)\n",
    "    return image_pairs, labels\n",
    "\n",
    "width = 256\n",
    "height = 192\n",
    "channels = 3\n",
    "def read_and_process_images(list_of_images):\n",
    "    X = []\n",
    "    for image in list_of_images:\n",
    "        X.append(cv2.imread(image, cv2.IMREAD_COLOR))\n",
    "    return X\n",
    "\n",
    "def read_and_process_image_pairs(list_of_image_pairs):\n",
    "    X = []\n",
    "    for image1, image2 in list_of_image_pairs:\n",
    "        img1 = cv2.imread(image1, cv2.IMREAD_COLOR)\n",
    "        img2 = cv2.imread(image2, cv2.IMREAD_COLOR)\n",
    "        img1 = cv2.resize(img1, (width, height), interpolation=cv2.INTER_AREA)\n",
    "        img2 = cv2.resize(img2, (width, height), interpolation=cv2.INTER_AREA)\n",
    "        X.append((img1, img2))\n",
    "    return X\n",
    "\n",
    "def traverse_datasets(hdf_file):\n",
    "    def h5py_dataset_iterator(g, prefix=''):\n",
    "        for key in g.keys():\n",
    "            item = g[key]\n",
    "            path = f'{prefix}/{key}'    \n",
    "            if isinstance(item, h5py.Dataset):\n",
    "                yield (path, item)\n",
    "            elif isinstance(item, h5py.Group):\n",
    "                yield from h5py_dataset_iterator(item, path)\n",
    "    with h5py.File(hdf_file, 'r') as f:\n",
    "        for path, _ in h5py_dataset_iterator(f):\n",
    "            yield path\n",
    "\n",
    "# def set_seed(seed):\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     tf.random.set_seed(seed)\n",
    "\n",
    "def train_and_evaluate(seed):\n",
    "    # set_seed(seed)\n",
    "\n",
    "    train_images, train_scores = get_scores(train_data_path)\n",
    "    test_images, test_scores = get_scores(test_data_path)\n",
    "\n",
    "    temp = list(zip(train_images, train_scores))\n",
    "    random.shuffle(temp)\n",
    "    train_images, train_scores = zip(*temp)\n",
    "\n",
    "    X_train = np.array(read_and_process_images(train_images))\n",
    "    y_train = np.array(train_scores)\n",
    "    X_val = np.array(read_and_process_images(test_images))\n",
    "    y_val = np.array(test_scores)\n",
    "\n",
    "    train_image_pairs, train_labels = get_image_pairs_and_labels(train_pairs_csv)\n",
    "    test_image_pairs, test_labels = get_image_pairs_and_labels(test_pairs_csv)\n",
    "\n",
    "    temp = list(zip(train_image_pairs, train_labels))\n",
    "    random.shuffle(temp)\n",
    "    train_image_pairs, train_labels = zip(*temp)\n",
    "\n",
    "    X_pairs_train = np.array(read_and_process_image_pairs(train_image_pairs))\n",
    "    y_pairs_train = np.array(train_labels)\n",
    "    X_pairs_val = np.array(read_and_process_image_pairs(test_image_pairs))\n",
    "    y_pairs_val = np.array(test_labels)\n",
    "\n",
    "    weights = {}\n",
    "    filename = '../../Calista/pretrainedModels/flickr_style.h5'\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        for dset in traverse_datasets(filename):\n",
    "            weights[dset] = f[dset][:]\n",
    "\n",
    "    conv1_bias = weights['/conv1/conv1/bias:0']\n",
    "    conv1_kernel = weights['/conv1/conv1/kernel:0']\n",
    "    conv2_bias = weights['/conv2/conv2/bias:0']\n",
    "    conv2_kernel = weights['/conv2/conv2/kernel:0']\n",
    "    conv3_bias = weights['/conv3/conv3/bias:0']\n",
    "    conv3_kernel = weights['/conv3/conv3/kernel:0']\n",
    "    conv4_bias = weights['/conv4/conv4/bias:0']\n",
    "    conv4_kernel = weights['/conv4/conv4/kernel:0']\n",
    "    conv5_bias = weights['/conv5/conv5/bias:0']\n",
    "    conv5_kernel = weights['/conv5/conv5/kernel:0']\n",
    "\n",
    "    class LRN(Layer):\n",
    "        def __init__(self, n=5, alpha=0.0001, beta=0.75, k=2, **kwargs):\n",
    "            self.n = n\n",
    "            self.alpha = alpha\n",
    "            self.beta = beta\n",
    "            self.k = k\n",
    "            super(LRN, self).__init__(**kwargs)\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.shape = input_shape\n",
    "            super(LRN, self).build(input_shape)\n",
    "\n",
    "        def call(self, x, mask=None):\n",
    "            if K.image_data_format() == \"channels_first\":\n",
    "                _, f, r, c = self.shape\n",
    "            else:\n",
    "                _, r, c, f = self.shape\n",
    "            half_n = self.n // 2\n",
    "            squared = tf.square(x)\n",
    "            pooled = tf.nn.pool(squared, window_shape=(half_n, half_n), pooling_type='AVG', strides=(1, 1),\n",
    "                            padding=\"SAME\")\n",
    "            if K.image_data_format() == \"channels_first\":\n",
    "                summed = tf.reduce_sum(pooled, axis=1, keepdims=True)\n",
    "                averaged = (self.alpha / self.n) * tf.repeat(summed, f, axis=1)\n",
    "            else:\n",
    "                summed = tf.reduce_sum(pooled, axis=3, keepdims=True)\n",
    "                averaged = (self.alpha / self.n) * tf.repeat(summed, f, axis=3)\n",
    "            denom = tf.pow(self.k + averaged, self.beta)\n",
    "            return x / denom\n",
    "\n",
    "        def get_output_shape_for(self, input_shape):\n",
    "            return input_shape\n",
    "\n",
    "    input_shape=(192, 256, 3)\n",
    "    l = 0.01  # Regularization factor\n",
    "    im_data = layers.Input(shape=input_shape, dtype='float32', name='im_data')\n",
    "\n",
    "    conv1 = layers.Conv2D(96, kernel_size=(11, 11), strides=(4, 4), name='conv1',\n",
    "                          activation='relu', input_shape=input_shape,\n",
    "                          kernel_regularizer=regularizers.l2(l))(im_data)\n",
    "\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\", name='pool1')(conv1)\n",
    "    norm1 = LRN(name='norm1')(pool1)\n",
    "    drop1 = layers.Dropout(0.1, name='dropout1')(norm1)\n",
    "\n",
    "    layer1_1 = layers.Lambda(lambda x: x[:, :, :, :48], name='split1_1')(drop1)\n",
    "    layer1_2 = layers.Lambda(lambda x: x[:, :, :, 48:], name='split1_2')(drop1)\n",
    "\n",
    "    conv2_1 = layers.Conv2D(128, kernel_size=(5, 5), strides=(1, 1),\n",
    "                            activation='relu', padding='same',\n",
    "                            name='conv2_1', kernel_regularizer=regularizers.l2(l))(layer1_1)\n",
    "\n",
    "    conv2_2 = layers.Conv2D(128, kernel_size=(5, 5), strides=(1, 1),\n",
    "                            activation='relu', padding='same',\n",
    "                            name='conv2_2', kernel_regularizer=regularizers.l2(l))(layer1_2)\n",
    "\n",
    "    conv2 = layers.Concatenate(name='conv_2')([conv2_1, conv2_2])\n",
    "\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool2')(conv2)\n",
    "    norm2 = LRN(name=\"norm2\")(pool2)\n",
    "    drop2 = layers.Dropout(0.1, name='dropout2')(norm2)\n",
    "\n",
    "    conv3 = layers.Conv2D(384, kernel_size=(3, 3), strides=(1, 1), activation='relu',\n",
    "                          name='conv3', padding='same',\n",
    "                          kernel_regularizer=regularizers.l2(l))(drop2)\n",
    "    drop3 = layers.Dropout(0.1, name='dropout3')(conv3)\n",
    "\n",
    "    layer3_1 = layers.Lambda(lambda x: x[:, :, :, :192], name='split3_1')(drop3)\n",
    "    layer3_2 = layers.Lambda(lambda x: x[:, :, :, 192:], name='split3_2')(drop3)\n",
    "\n",
    "    conv4_1 = layers.Conv2D(192, kernel_size=(3, 3), strides=(1, 1),\n",
    "                            activation='relu', padding='same',\n",
    "                            name='conv4_1', kernel_regularizer=regularizers.l2(l))(layer3_1)\n",
    "\n",
    "    conv4_2 = layers.Conv2D(192, kernel_size=(3, 3), strides=(1, 1),\n",
    "                            activation='relu', padding='same',\n",
    "                            name='conv4_2', kernel_regularizer=regularizers.l2(l))(layer3_2)\n",
    "\n",
    "    conv4 = layers.Concatenate(name='conv_4')([conv4_1, conv4_2])\n",
    "\n",
    "    layer4_1 = layers.Lambda(lambda x: x[:, :, :, :192], name='split4_1')(conv4)\n",
    "    layer4_2 = layers.Lambda(lambda x: x[:, :, :, 192:], name='split4_2')(conv4)\n",
    "\n",
    "    conv5_1 = layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1),\n",
    "                            activation='relu', padding='same',\n",
    "                            name='conv5_1', kernel_regularizer=regularizers.l2(l))(layer4_1)\n",
    "\n",
    "    conv5_2 = layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1),\n",
    "                            activation='relu', padding='same',\n",
    "                            name='conv5_2', kernel_regularizer=regularizers.l2(l))(layer4_2)\n",
    "\n",
    "    conv5 = layers.Concatenate(name='conv_5')([conv5_1, conv5_2])\n",
    "\n",
    "    pool5 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(conv5)\n",
    "\n",
    "    shared_model = models.Model(inputs=im_data, outputs=pool5, name='shared_feature_extractor')\n",
    "\n",
    "    flat = layers.Flatten()(shared_model.output)\n",
    "    fc6 = layers.Dense(1024, activation='relu', name='Ratingfc6',\n",
    "                       kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "                       bias_initializer='zeros',\n",
    "                       kernel_regularizer=regularizers.l2(l))(flat)\n",
    "    drop6 = layers.Dropout(0.5, name='RatingDropout6')(fc6)\n",
    "\n",
    "    fc7 = layers.Dense(512, activation='relu', name='Ratingfc7',\n",
    "                       kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "                       bias_initializer='zeros',\n",
    "                       kernel_regularizer=regularizers.l2(l))(drop6)\n",
    "    drop7 = layers.Dropout(0.5, name='RatingDropout7')(fc7)\n",
    "\n",
    "    fc8 = layers.Dense(1, name='rating_output',\n",
    "                       kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "                       bias_initializer='zeros')(drop7)\n",
    "\n",
    "    rating_model = models.Model(inputs=shared_model.input, outputs=fc8, name='rating_model')\n",
    "\n",
    "    input_shape = shared_model.input_shape[1:]\n",
    "        \n",
    "    input_a = layers.Input(shape=input_shape, dtype='float32', name='input_a')\n",
    "    input_b = layers.Input(shape=input_shape, dtype='float32', name='input_b')\n",
    "\n",
    "    output_a = shared_model(input_a)\n",
    "    output_b = shared_model(input_b)\n",
    "\n",
    "    flat_a = layers.Flatten()(output_a)\n",
    "    flat_b = layers.Flatten()(output_b)\n",
    "\n",
    "    shared_fc6 = layers.Dense(1024, activation='relu', name='Comparisonfc6', kernel_regularizer=regularizers.l2(l),\n",
    "                              kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')\n",
    "    shared_drop6 = layers.Dropout(0.5, name='ComparisonDropout6')\n",
    "    shared_fc7 = layers.Dense(512, activation='relu', name='Comparisonfc7', kernel_regularizer=regularizers.l2(l),\n",
    "                              kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')\n",
    "    shared_drop7 = layers.Dropout(0.5, name='ComparisonDropout7')\n",
    "    shared_fc8 = layers.Dense(1, name='Comparisonfc8',\n",
    "                              kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01), bias_initializer='zeros')\n",
    "\n",
    "    fc6_a = shared_fc6(flat_a)\n",
    "    drop6_a = shared_drop6(fc6_a)\n",
    "    fc7_a = shared_fc7(drop6_a)\n",
    "    drop7_a = shared_drop7(fc7_a)\n",
    "    fc8_a = shared_fc8(drop7_a)\n",
    "\n",
    "    fc6_b = shared_fc6(flat_b)\n",
    "    drop6_b = shared_drop6(fc6_b)\n",
    "    fc7_b = shared_fc7(drop6_b)\n",
    "    drop7_b = shared_drop7(fc7_b)\n",
    "    fc8_b = shared_fc8(drop7_b)\n",
    "\n",
    "    difference = layers.Subtract(name='Difference')([fc8_a, fc8_b])\n",
    "    comparison_output = layers.Activation('tanh', name='comparison_output')(difference)\n",
    "\n",
    "    comparison_model = models.Model(inputs=[input_a, input_b], outputs=comparison_output, name='comparison_model')\n",
    "\n",
    "    rating_input = layers.Input(shape=(192, 256, 3), name='rating_input')\n",
    "    comparison_input_a = layers.Input(shape=(192, 256, 3), name='comparison_input_a')\n",
    "    comparison_input_b = layers.Input(shape=(192, 256, 3), name='comparison_input_b')\n",
    "\n",
    "    rating_output = rating_model(rating_input)\n",
    "    comparison_output = comparison_model([comparison_input_a, comparison_input_b])\n",
    "\n",
    "    joint_model = models.Model(\n",
    "        inputs=[rating_input, comparison_input_a, comparison_input_b],\n",
    "        outputs=[rating_output, comparison_output],\n",
    "        name='joint_model'\n",
    "    )\n",
    "\n",
    "\n",
    "    shared_model.get_layer('conv1').set_weights([conv1_kernel, conv1_bias])\n",
    "    shared_model.get_layer('conv2_1').set_weights([conv2_kernel[:, :, :, :128], conv2_bias[:128]])\n",
    "    shared_model.get_layer('conv2_2').set_weights([conv2_kernel[:, :, :, 128:], conv2_bias[128:]])\n",
    "    shared_model.get_layer('conv3').set_weights([conv3_kernel, conv3_bias])\n",
    "    shared_model.get_layer('conv4_1').set_weights([conv4_kernel[:, :, :, :192], conv4_bias[:192]])\n",
    "    shared_model.get_layer('conv4_2').set_weights([conv4_kernel[:, :, :, 192:], conv4_bias[192:]])\n",
    "    shared_model.get_layer('conv5_1').set_weights([conv5_kernel[:, :, :, :128], conv5_bias[:128]])\n",
    "    shared_model.get_layer('conv5_2').set_weights([conv5_kernel[:, :, :, 128:], conv5_bias[128:]])\n",
    "\n",
    "    # print(\"\\nShared Feature Extractor Summary:\")\n",
    "    # shared_model.summary()\n",
    "\n",
    "    # print(\"\\nRating Model Summary:\")\n",
    "    # rating_model.summary()\n",
    "\n",
    "    # print(\"\\nComparison Model Summary:\")\n",
    "    # comparison_model.summary()\n",
    "\n",
    "    # print(\"Joint Model Summary:\")\n",
    "    # joint_model.summary()\n",
    "\n",
    "#     def rating_rmse(y_true, y_pred):\n",
    "#         y_true = tf.cast(y_true, tf.float32)\n",
    "#         y_pred = tf.cast(y_pred, tf.float32)\n",
    "#         return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "#     def custom_binary_accuracy(y_true, y_pred):\n",
    "#         y_pred = tf.where(y_pred >= 0.0, 1.0, -1.0)\n",
    "#         y_true = tf.cast(y_true, tf.float32)\n",
    "#         return tf.reduce_mean(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n",
    "\n",
    "#     def mse_regression_loss(y_true, y_pred):\n",
    "#         return tf.reduce_mean(tf.square(y_pred - y_true))\n",
    "\n",
    "#     def bradley_terry_loss(y_true, y_pred):\n",
    "#         y_true = tf.cast(y_true, tf.float32)\n",
    "#         y_pred = tf.cast(y_pred, tf.float32)\n",
    "#         return tf.reduce_mean(tf.math.log(1 + tf.exp(-y_true * y_pred)))\n",
    "\n",
    "#     batch_size = 32\n",
    "#     class DataGenerator(tf.keras.utils.Sequence):\n",
    "#         def __init__(self, X_rating, y_rating, X_pairs, y_pairs, batch_size, normalize=True):\n",
    "#             self.X_rating = X_rating\n",
    "#             self.y_rating = y_rating\n",
    "#             self.X_pairs = X_pairs\n",
    "#             self.y_pairs = y_pairs\n",
    "#             self.batch_size = batch_size\n",
    "#             self.normalize = normalize\n",
    "#             self.indexes = np.arange(len(self.X_rating))\n",
    "#             self.on_epoch_end()\n",
    "            \n",
    "#         def __len__(self):\n",
    "#             return int(np.floor(len(self.X_rating) / self.batch_size))\n",
    "\n",
    "#         def __getitem__(self, index):\n",
    "#             batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "            \n",
    "#             X_rating_batch = self.X_rating[batch_indexes]\n",
    "#             y_rating_batch = self.y_rating[batch_indexes]\n",
    "            \n",
    "#             X_pairs_batch = self.X_pairs[batch_indexes]\n",
    "#             y_pairs_batch = self.y_pairs[batch_indexes]\n",
    "            \n",
    "#             comparison_input_a = np.array([pair[0] for pair in X_pairs_batch])\n",
    "#             comparison_input_b = np.array([pair[1] for pair in X_pairs_batch])\n",
    "            \n",
    "#             if self.normalize:\n",
    "#                 X_rating_batch = self.normalize_images(X_rating_batch)\n",
    "#                 comparison_input_a = self.normalize_images(comparison_input_a)\n",
    "#                 comparison_input_b = self.normalize_images(comparison_input_b)\n",
    "            \n",
    "#             return [X_rating_batch, comparison_input_a, comparison_input_b], [y_rating_batch, y_pairs_batch]\n",
    "        \n",
    "#         def on_epoch_end(self):\n",
    "#             np.random.shuffle(self.indexes)\n",
    "        \n",
    "#         def normalize_images(self, images):\n",
    "#             return images / 255.0\n",
    "\n",
    "#     train_generator = DataGenerator(X_train, y_train, X_pairs_train, y_pairs_train, batch_size)\n",
    "#     val_generator = DataGenerator(X_val, y_val, X_pairs_val, y_pairs_val, batch_size)\n",
    "\n",
    "#     epochs = 95\n",
    "#     base_lr = 1e-3\n",
    "#     initial_learning_rate = base_lr\n",
    "#     lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#         initial_learning_rate,\n",
    "#         decay_steps=100000,\n",
    "#         decay_rate=0.96,\n",
    "#         staircase=True)\n",
    "#     sgd = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9, nesterov=True)\n",
    "\n",
    "#     get_custom_objects().update({\n",
    "#         'mse_regression_loss': mse_regression_loss,\n",
    "#         'bradley_terry_loss': bradley_terry_loss,\n",
    "#         'rating_rmse': rating_rmse,\n",
    "#         'custom_binary_accuracy': custom_binary_accuracy\n",
    "#     })\n",
    "\n",
    "#     joint_model.compile(\n",
    "#         optimizer=sgd, \n",
    "#         loss={\n",
    "#             'rating_model': 'mse_regression_loss', \n",
    "#             'comparison_model': 'bradley_terry_loss'\n",
    "#         },\n",
    "#         loss_weights={\n",
    "#             'rating_model': 0.6,\n",
    "#             'comparison_model': 1\n",
    "#         },  \n",
    "#         metrics={\n",
    "#             'rating_model': 'rating_rmse', \n",
    "#             'comparison_model': 'custom_binary_accuracy'\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     history = joint_model.fit(\n",
    "#         train_generator,\n",
    "#         validation_data=val_generator,\n",
    "#         epochs=epochs\n",
    "#     )\n",
    "\n",
    "#     rating_predictions = []\n",
    "#     X_val_norm = X_val / 255.0\n",
    "#     for img in X_val_norm:\n",
    "#         img = img.reshape(1, 192, 256, 3)\n",
    "#         pred = rating_model.predict(img)\n",
    "#         rating_predictions.append(float(pred))\n",
    "#     rating_predictions = np.array(rating_predictions)\n",
    "\n",
    "#     corr, p, lo, hi = pearsonr_ci(y_val, rating_predictions)\n",
    "#     rmse_test = sqrt(mean_squared_error(y_val, rating_predictions))\n",
    "\n",
    "#     X_pairs_val_normalized = normalize_image_pairs(X_pairs_val)\n",
    "#     comparison_predictions = []\n",
    "#     for img1, img2 in X_pairs_val_normalized:\n",
    "#         img1_norm = np.expand_dims(img1, axis=0)\n",
    "#         img2_norm = np.expand_dims(img2, axis=0)\n",
    "#         pred = comparison_model.predict([img1_norm, img2_norm])\n",
    "#         comparison_predictions.append(float(pred[0][0]))\n",
    "#     comparison_predictions = np.array(comparison_predictions)\n",
    "#     comparison_accuracy = custom_binary_accuracy(y_pairs_val, comparison_predictions).numpy()\n",
    "\n",
    "#     return corr, comparison_accuracy, joint_model\n",
    "\n",
    "# def normalize_image_pairs(X_pairs):\n",
    "#     normalized_pairs = []\n",
    "#     for img1, img2 in X_pairs:\n",
    "#         img1_normalized = img1 / 255.0\n",
    "#         img2_normalized = img2 / 255.0\n",
    "#         normalized_pairs.append((img1_normalized, img2_normalized))\n",
    "#     return normalized_pairs\n",
    "\n",
    "# def pearsonr_ci(x, y, alpha=0.05):\n",
    "#     N = len(x)\n",
    "#     r, p = stats.pearsonr(x, y)\n",
    "#     r_z = np.arctanh(r)\n",
    "#     se = 1/np.sqrt(N-3)\n",
    "#     z = stats.norm.ppf(1-alpha/2)\n",
    "#     lo_z, hi_z = r_z-z*se, r_z+z*se\n",
    "#     lo, hi = np.tanh((lo_z, hi_z))\n",
    "#     return r, p, lo, hi   \n",
    "\n",
    "# best_corr = -np.inf\n",
    "# best_accuracy = -np.inf\n",
    "# best_seed = None\n",
    "# best_model = None\n",
    "\n",
    "# # Define the threshold values\n",
    "# pearson_threshold = 0.78\n",
    "# accuracy_threshold = 0.69\n",
    "\n",
    "# for seed in range(413, 419):  # Adjust the range as needed\n",
    "#     corr, accuracy, model = train_and_evaluate(seed)\n",
    "#     print(f\"Seed: {seed}, Pearson Correlation: {corr:.4f}, Comparison Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "#     if corr >= pearson_threshold:\n",
    "#         print(f\"\\nSeed with Pearson Correlation >= {pearson_threshold}: {seed}\")\n",
    "#         print(f\"Pearson Correlation: {corr:.4f}\")\n",
    "#         print(f\"Comparison Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "#     if accuracy >= accuracy_threshold:\n",
    "#         print(f\"\\nSeed with Comparison Accuracy >= {accuracy_threshold}: {seed}\")\n",
    "#         print(f\"Pearson Correlation: {corr:.4f}\")\n",
    "#         print(f\"Comparison Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# # Save the best model\n",
    "# best_model.save('best_joint_model.h5')\n",
    "\n",
    "# # Extract RMSE metrics from history\n",
    "# rating_rmse = history.history['rating_model_rating_rmse']\n",
    "# val_rating_rmse = history.history['val_rating_model_rating_rmse']\n",
    "\n",
    "# epochs_x = range(1, len(rating_rmse) + 1)\n",
    "\n",
    "# # Plot the RMSE learning curves for the rating task\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(epochs_x, rating_rmse, 'b', label='Training RMSE (Rating Task)')\n",
    "# plt.plot(epochs_x, val_rating_rmse, 'r', label='Validation RMSE (Rating Task)')\n",
    "# plt.title('RMSE Learning Curve - Rating Task')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('RMSE')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Extract RMSE metrics from history\n",
    "# comparison_output_custom_binary_accuracy = history.history['comparison_model_custom_binary_accuracy']\n",
    "# val_comparison_output_custom_binary_accuracy = history.history['val_comparison_model_custom_binary_accuracy']\n",
    "\n",
    "\n",
    "# epochs_x = range(1, len(comparison_output_custom_binary_accuracy) + 1)\n",
    "\n",
    "# # Plot the RMSE learning curves for the comparison task\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.plot(epochs_x, comparison_output_custom_binary_accuracy, 'b', label='Training accuracy (Comparison Task)')\n",
    "# plt.plot(epochs_x, val_comparison_output_custom_binary_accuracy, 'r', label='Validation accuracy (Comparison Task)')\n",
    "# plt.title('Accuracy Learning Curve - Comparison Task')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# from scipy import stats\n",
    "\n",
    "# def pearsonr_ci(x,y,alpha=0.05):\n",
    "#     ''' calculate Pearson correlation along with the confidence interval using scipy and numpy\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     x, y : iterable object such as a list or np.array\n",
    "#       Input for correlation calculation\n",
    "#     alpha : float\n",
    "#       Significance level. 0.05 by default\n",
    "#     Returns\n",
    "#     -------\n",
    "#     r : float\n",
    "#       Pearson's correlation coefficient\n",
    "#     pval : float\n",
    "#       The corresponding p value\n",
    "#     lo, hi : float\n",
    "#       The lower and upper bound of confidence intervals\n",
    "#     '''\n",
    "#     N = len(x)\n",
    "#     r, p = stats.pearsonr(x,y)\n",
    "#     r_z = np.arctanh(r)\n",
    "#     se = 1/np.sqrt(N-3)\n",
    "#     z = stats.norm.ppf(1-alpha/2)\n",
    "#     lo_z, hi_z = r_z-z*se, r_z+z*se\n",
    "#     lo, hi = np.tanh((lo_z, hi_z))\n",
    "#     return r, p, lo, hi\n",
    "\n",
    "\n",
    "#     # Rating Model\n",
    "# rating_predictions = []\n",
    "\n",
    "# X_val = X_val / 255.0\n",
    "# for img in X_val:\n",
    "#   img = img.reshape(1, 192, 256, 3)\n",
    "#   pred = rating_model.predict(img)\n",
    "#   rating_predictions.append(float(pred))\n",
    "\n",
    "# rating_predictions = np.array(rating_predictions)\n",
    "\n",
    "\n",
    "# image_ids = [87, 45, 49, 94, 14, 83] # test image IDs sorted in descending order according to the website's aesthetics level\n",
    "\n",
    "# fig = plt.figure(figsize=(12, 16))\n",
    "# i = 1\n",
    "# for id in image_ids:\n",
    "#   if 'english' in test_images[id]:\n",
    "#     path = images_path + '/english_resized/' + test_images[id].rsplit('/', 1)[1]\n",
    "#   else:\n",
    "#     path = images_path + '/foreign_resized/' + test_images[id].rsplit('/', 1)[1]\n",
    "\n",
    "#   plt.subplot(len(image_ids)//2, 2, i)\n",
    "#   img = mping.imread(path)\n",
    "#   plt.title('User average rating: ' + str(np.round(y_val[id],2)) + '\\nPredicted rating: ' + str(np.round(rating_predictions[id],2)) + '\\n(' + chr(97+i-1) + ')', y=-0.25)\n",
    "#   plt.axis('off')\n",
    "#   plt.imshow(img)\n",
    "\n",
    "#   i += 1\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# from numpy.polynomial.polynomial import polyfit\n",
    "# b, m = polyfit(y_val, rating_predictions, 1)\n",
    "\n",
    "# fig = plt.figure()\n",
    "# plt.scatter(y_val, rating_predictions, c='c')\n",
    "# plt.plot(y_val, b + m * y_val, '-', c='b')\n",
    "# plt.xlabel('User ratings')\n",
    "# plt.ylabel('Predicted ratings')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from math import sqrt\n",
    "\n",
    "# corr, p, lo, hi = pearsonr_ci(y_val, rating_predictions)\n",
    "# print('Pearsons correlation: r=%.2f, p=%.2e, CI=[%.2f, %.2f]' % (corr, p, lo, hi))\n",
    "# rmse_test = sqrt(mean_squared_error(y_val, rating_predictions))\n",
    "# print('RMSE: %.3f' % rmse_test)\n",
    "\n",
    "# # # Comparison Model\n",
    "\n",
    "# # Normalize image pairs\n",
    "# def normalize_image_pairs(X_pairs):\n",
    "#     normalized_pairs = []\n",
    "#     for img1, img2 in X_pairs:\n",
    "#         img1_normalized = img1 / 255.0\n",
    "#         img2_normalized = img2 / 255.0\n",
    "#         normalized_pairs.append((img1_normalized, img2_normalized))\n",
    "#     return normalized_pairs\n",
    "\n",
    "# # Normalize the validation image pairs\n",
    "# X_pairs_val_normalized = normalize_image_pairs(X_pairs_val)\n",
    "\n",
    "# # Predict\n",
    "# comparison_predictions = []\n",
    "\n",
    "# for img1, img2 in X_pairs_val_normalized:\n",
    "#     # Add batch dimension\n",
    "#     img1_normalized = np.expand_dims(img1, axis=0)\n",
    "#     img2_normalized = np.expand_dims(img2, axis=0)\n",
    "    \n",
    "#     # Predict\n",
    "#     pred = comparison_model.predict([img1_normalized, img2_normalized])\n",
    "#     comparison_predictions.append(float(pred[0][0]))\n",
    "\n",
    "# comparison_predictions = np.array(comparison_predictions)\n",
    "\n",
    "# # Manually select specific IDs\n",
    "# manual_pair_ids = [24, 56, 62, 45, 16, 87]  # Replace with desired IDs\n",
    "\n",
    "# # Plotting\n",
    "# fig, axes = plt.subplots(len(manual_pair_ids), 2, figsize=(12, len(manual_pair_ids) * 4))\n",
    "# fig.subplots_adjust(hspace=0.4)\n",
    "# i = 0\n",
    "# for idx, id in enumerate(manual_pair_ids):\n",
    "#     img1, img2 = X_pairs_val[id]\n",
    "    \n",
    "#     # Plot first image of the pair\n",
    "#     axes[idx, 0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "#     axes[idx, 0].axis('off')\n",
    "#     axes[idx, 0].set_title('Image a', fontsize=12, pad=10)\n",
    "    \n",
    "#     # Plot second image of the pair\n",
    "#     axes[idx, 1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "#     axes[idx, 1].axis('off')\n",
    "#     axes[idx, 1].set_title('Image b', fontsize=12, pad=10)\n",
    "    \n",
    "#     # Add text with the true label and predicted value in the middle between the images\n",
    "#     fig.text(0.5, (len(manual_pair_ids) - idx - 0.5) / len(manual_pair_ids), \n",
    "#              f'True Label: {y_pairs_val[id]}\\nPredicted value: {comparison_predictions[id]:.2f}', \n",
    "#              ha='center', va='center', fontsize=14)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# # Calculate custom binary accuracy\n",
    "# accuracy = custom_binary_accuracy(y_pairs_val, comparison_predictions)\n",
    "\n",
    "# print(f\"Custom Binary Accuracy: {accuracy.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# # Open the HDF5 file\n",
    "# filename = 'best_joint_model.h5'\n",
    "# with h5py.File(filename, 'r') as f:\n",
    "#     # Function to recursively print the content of the HDF5 file\n",
    "#     def print_attrs(name, obj):\n",
    "#         print(name)\n",
    "#         for key, val in obj.attrs.items():\n",
    "#             print(f\"    {key}: {val}\")\n",
    "\n",
    "#     # Print the structure of the HDF5 file\n",
    "#     f.visititems(print_attrs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Define the Data Generators for each task\n",
    "# class RatingDataGenerator(tf.keras.utils.Sequence):\n",
    "#     def __init__(self, X, y, batch_size, normalize=True):\n",
    "#         self.X = X\n",
    "#         self.y = y\n",
    "#         self.batch_size = batch_size\n",
    "#         self.normalize = normalize\n",
    "#         self.indexes = np.arange(len(self.X))\n",
    "#         self.on_epoch_end()\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return int(np.floor(len(self.X) / self.batch_size))\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "#         X_batch = self.X[batch_indexes]\n",
    "#         y_batch = self.y[batch_indexes]\n",
    "        \n",
    "#         if self.normalize:\n",
    "#             X_batch = self.normalize_images(X_batch)\n",
    "        \n",
    "#         return X_batch, y_batch\n",
    "    \n",
    "#     def on_epoch_end(self):\n",
    "#         np.random.shuffle(self.indexes)\n",
    "    \n",
    "#     def normalize_images(self, images):\n",
    "#         return images / 255.0\n",
    "\n",
    "# class ComparisonDataGenerator(tf.keras.utils.Sequence):\n",
    "#     def __init__(self, X_pairs, y_pairs, batch_size, normalize=True):\n",
    "#         self.X_pairs = X_pairs\n",
    "#         self.y_pairs = y_pairs\n",
    "#         self.batch_size = batch_size\n",
    "#         self.normalize = normalize\n",
    "#         self.indexes = np.arange(len(self.X_pairs))\n",
    "#         self.on_epoch_end()\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return int(np.floor(len(self.X_pairs) / self.batch_size))\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "#         X_pairs_batch = [self.X_pairs[k] for k in batch_indexes]\n",
    "#         y_pairs_batch = self.y_pairs[batch_indexes]\n",
    "        \n",
    "#         comparison_input_a = np.array([pair[0] for pair in X_pairs_batch])\n",
    "#         comparison_input_b = np.array([pair[1] for pair in X_pairs_batch])\n",
    "        \n",
    "#         if self.normalize:\n",
    "#             comparison_input_a = self.normalize_images(comparison_input_a)\n",
    "#             comparison_input_b = self.normalize_images(comparison_input_b)\n",
    "        \n",
    "#         return [comparison_input_a, comparison_input_b], y_pairs_batch\n",
    "    \n",
    "#     def on_epoch_end(self):\n",
    "#         np.random.shuffle(self.indexes)\n",
    "    \n",
    "#     def normalize_images(self, images):\n",
    "#         return images / 255.0\n",
    "\n",
    "# # Define the combined generator with reinitialization logic\n",
    "# class CombinedDataGenerator(tf.keras.utils.Sequence):\n",
    "#     def __init__(self, rating_gen, comparison_gen, rating_batch_size, comparison_batch_size):\n",
    "#         self.rating_gen = rating_gen\n",
    "#         self.comparison_gen = comparison_gen\n",
    "#         self.rating_batch_size = rating_batch_size\n",
    "#         self.comparison_batch_size = comparison_batch_size\n",
    "#         self.rating_iter = iter(self.rating_gen)\n",
    "#         self.comparison_iter = iter(self.comparison_gen)\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         # return min(len(self.rating_gen), len(self.comparison_gen))\n",
    "#         return max(len(self.rating_gen), len(self.comparison_gen))\n",
    "\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         try:\n",
    "#             rating_data, rating_labels = next(self.rating_iter)\n",
    "#         except StopIteration:\n",
    "#             self.rating_iter = iter(self.rating_gen)\n",
    "#             rating_data, rating_labels = next(self.rating_iter)\n",
    "        \n",
    "#         try:\n",
    "#             comparison_data, comparison_labels = next(self.comparison_iter)\n",
    "#         except StopIteration:\n",
    "#             self.comparison_iter = iter(self.comparison_gen)\n",
    "#             comparison_data, comparison_labels = next(self.comparison_iter)\n",
    "        \n",
    "#         return [rating_data, comparison_data[0], comparison_data[1]], [rating_labels, comparison_labels]\n",
    "    \n",
    "#     def on_epoch_end(self):\n",
    "#         self.rating_gen.on_epoch_end()\n",
    "#         self.comparison_gen.on_epoch_end()\n",
    "#         self.rating_iter = iter(self.rating_gen)\n",
    "#         self.comparison_iter = iter(self.comparison_gen)\n",
    "\n",
    "# # Create data generators for each task\n",
    "# rating_batch_size = 32\n",
    "# comparison_batch_size = 64  # Larger batch size for the comparison task\n",
    "\n",
    "# rating_gen_train = RatingDataGenerator(X_train, y_train, rating_batch_size)\n",
    "# comparison_gen_train = ComparisonDataGenerator(X_pairs_train, y_pairs_train, comparison_batch_size)\n",
    "# rating_gen_val = RatingDataGenerator(X_val, y_val, rating_batch_size)\n",
    "# comparison_gen_val = ComparisonDataGenerator(X_pairs_val, y_pairs_val, comparison_batch_size)\n",
    "\n",
    "# # Create the combined generators for training and validation\n",
    "# train_generator = CombinedDataGenerator(rating_gen_train, comparison_gen_train, rating_batch_size, comparison_batch_size)\n",
    "# val_generator = CombinedDataGenerator(rating_gen_val, comparison_gen_val, rating_batch_size, comparison_batch_size)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old rating data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import rating_data_generator\n",
    "\n",
    "\n",
    "# Define the output signature for the rating generator\n",
    "rating_output_signature = (\n",
    "    tf.TensorSpec(shape=(None, 192, 256, 3), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(None,), dtype=tf.float32)\n",
    ")\n",
    "\n",
    "# Create TensorFlow Dataset objects for rating task\n",
    "rating_gen_train = tf.data.Dataset.from_generator(\n",
    "    lambda: rating_data_generator(X_train, y_train, batch_size),\n",
    "    output_signature=rating_output_signature\n",
    ")\n",
    "\n",
    "rating_gen_val = tf.data.Dataset.from_generator(\n",
    "    lambda: rating_data_generator(X_val, y_val, batch_size),\n",
    "    output_signature=rating_output_signature\n",
    ") \n",
    "#repeat???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attempt for weighted grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "# def pearsonr_ci(x,y,alpha=0.05):\n",
    "#     ''' calculate Pearson correlation along with the confidence interval using scipy and numpy\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     x, y : iterable object such as a list or np.array\n",
    "#       Input for correlation calculation\n",
    "#     alpha : float\n",
    "#       Significance level. 0.05 by default\n",
    "#     Returns\n",
    "#     -------\n",
    "#     r : float\n",
    "#       Pearson's correlation coefficient\n",
    "#     pval : float\n",
    "#       The corresponding p value\n",
    "#     lo, hi : float\n",
    "#       The lower and upper bound of confidence intervals\n",
    "#     '''\n",
    "#     N = len(x)\n",
    "#     r, p = stats.pearsonr(x,y)\n",
    "#     r_z = np.arctanh(r)\n",
    "#     se = 1/np.sqrt(N-3)\n",
    "#     z = stats.norm.ppf(1-alpha/2)\n",
    "#     lo_z, hi_z = r_z-z*se, r_z+z*se\n",
    "#     lo, hi = np.tanh((lo_z, hi_z))\n",
    "#     return r, p, lo, hi\n",
    "\n",
    "#     # Normalize image pairs\n",
    "# def normalize_image_pairs(X_pairs):\n",
    "#     normalized_pairs = []\n",
    "#     for img1, img2 in X_pairs:\n",
    "#         img1_normalized = img1 / 255.0\n",
    "#         img2_normalized = img2 / 255.0\n",
    "#         normalized_pairs.append((img1_normalized, img2_normalized))\n",
    "#     return normalized_pairs\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# class MetricsCallback(Callback):\n",
    "#     def __init__(self, X_val, y_val, X_pairs_val, y_pairs_val, rating_model, comparison_model, start_epoch=130):\n",
    "#         super().__init__()\n",
    "#         self.X_val = X_val\n",
    "#         self.y_val = y_val\n",
    "#         self.X_pairs_val = X_pairs_val\n",
    "#         self.y_pairs_val = y_pairs_val\n",
    "#         self.rating_model = rating_model\n",
    "#         self.comparison_model = comparison_model\n",
    "#         self.best_pearson_epoch = -1\n",
    "#         self.best_accuracy_epoch = -1\n",
    "#         self.best_pearson = -np.inf\n",
    "#         self.best_accuracy = -np.inf\n",
    "#         self.start_epoch = start_epoch  # Interval to calculate metrics\n",
    "#         self.epochs_with_thresholds = []  # To store epochs meeting both criteria\n",
    "#         self.pearson_values = []  # Store Pearson correlation values for each epoch\n",
    "#         self.accuracy_values = []  # Store accuracy values for each epoch\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         if (epoch + 1) > self.start_epoch:\n",
    "\n",
    "#             # Normalize the validation data for rating model\n",
    "#             X_val_norm = self.X_val / 255.0\n",
    "            \n",
    "#             # Predict ratings on the validation data\n",
    "#             rating_predictions = []\n",
    "#             for img in X_val_norm:\n",
    "#                 img = img.reshape(1, 192, 256, 3)\n",
    "#                 pred = self.rating_model.predict(img)\n",
    "#                 rating_predictions.append(float(pred))\n",
    "#             rating_predictions = np.array(rating_predictions)\n",
    "            \n",
    "#             # Calculate Pearson correlation\n",
    "#             corr, p, lo, hi = pearsonr_ci(self.y_val, rating_predictions)\n",
    "#             rmse_test = np.sqrt(mean_squared_error(self.y_val, rating_predictions))\n",
    "            \n",
    "#             # Track the best Pearson correlation\n",
    "#             if corr > self.best_pearson:\n",
    "#                 self.best_pearson = corr\n",
    "#                 self.best_pearson_epoch = epoch + 1\n",
    "            \n",
    "#             # Normalize and predict for comparison model\n",
    "#             X_pairs_val_normalized = normalize_image_pairs(self.X_pairs_val)\n",
    "#             comparison_predictions = []\n",
    "#             for img1, img2 in X_pairs_val_normalized:\n",
    "#                 img1_norm = np.expand_dims(img1, axis=0)\n",
    "#                 img2_norm = np.expand_dims(img2, axis=0)\n",
    "#                 pred = self.comparison_model.predict([img1_norm, img2_norm])\n",
    "#                 comparison_predictions.append(float(pred[0][0]))\n",
    "#             comparison_predictions = np.array(comparison_predictions)\n",
    "            \n",
    "#             # Calculate accuracy\n",
    "#             accuracy = custom_binary_accuracy(self.y_pairs_val, comparison_predictions).numpy()\n",
    "            \n",
    "#             # Track the best accuracy\n",
    "#             if accuracy > self.best_accuracy:\n",
    "#                 self.best_accuracy = accuracy\n",
    "#                 self.best_accuracy_epoch = epoch + 1\n",
    "            \n",
    "#             # Store Pearson correlation and accuracy in logs\n",
    "#             logs['val_pearson_correlation'] = corr\n",
    "#             logs['val_rmse'] = rmse_test\n",
    "#             logs['val_accuracy'] = accuracy\n",
    "            \n",
    "#             # Store the values for each epoch\n",
    "#             self.pearson_values.append(corr)\n",
    "#             self.accuracy_values.append(accuracy)\n",
    "            \n",
    "#             print(f'Epoch {epoch + 1} - val_pearson_correlation: {corr:.4f} (p={p:.2e}, CI=[{lo:.2f}, {hi:.2f}]) - val_rmse: {rmse_test:.4f} - val_accuracy: {accuracy:.4f}')\n",
    "\n",
    "#             # Check if both thresholds are met\n",
    "#             if corr >= 0.77 and accuracy >= 0.69:\n",
    "#                 self.epochs_with_thresholds.append(epoch + 1)\n",
    "#                 # Print the Pearson correlation and accuracy for the epoch\n",
    "#                 print(f'Epoch {epoch + 1} met the thresholds - Pearson Correlation: {corr:.4f}, Accuracy: {accuracy:.4f}')\n",
    "        \n",
    "#     def on_train_end(self, logs=None):\n",
    "#         print(f'\\nBest Pearson Correlation: {self.best_pearson:.4f} at epoch {self.best_pearson_epoch}')\n",
    "#         print(f'Best Accuracy: {self.best_accuracy:.4f} at epoch {self.best_accuracy_epoch}')\n",
    "\n",
    "#         # Print epochs where both thresholds were met\n",
    "#         if self.epochs_with_thresholds:\n",
    "#             print(f'Epochs where Pearson >= 0.77 and Accuracy >= 0.69: {self.epochs_with_thresholds}')\n",
    "#         else:\n",
    "#             print('No epochs where both Pearson >= 0.77 and Accuracy >= 0.69 were met.')\n",
    "\n",
    "# from itertools import product\n",
    "\n",
    "# # Define weight combinations for the rating_model and comparison_model\n",
    "# weight_combinations = [(0.5, 0.5)]\n",
    "\n",
    "# # Store results for each weight combination\n",
    "# results = []\n",
    "\n",
    "# with open('output_log.txt', 'a') as f:\n",
    "\n",
    "#     # Loop through each weight combination\n",
    "#     for rating_weight, comparison_weight in weight_combinations:\n",
    "        \n",
    "#         # Print and log the starting message\n",
    "#         msg = f'\\nTraining with rating_model weight: {rating_weight}, comparison_model weight: {comparison_weight}'\n",
    "#         print(msg)\n",
    "#         f.write(msg + \"\\n\")\n",
    "#         f.flush()  # Ensure the message is written to the file immediately\n",
    "#         # print(f'\\nTraining with rating_model weight: {rating_weight}, comparison_model weight: {comparison_weight}')\n",
    "\n",
    "\n",
    "#         # Reinitialize the shared feature extractor\n",
    "#         model_base = SharedFeatureExtractor()\n",
    "#         shared_feature_extractor = model_base.build_model()\n",
    "        \n",
    "#         # Reinitialize the rating model using the shared feature extractor\n",
    "#         rating_task = RatingModel(shared_feature_extractor)\n",
    "#         rating_model = rating_task.build_model()\n",
    "        \n",
    "#         # Reinitialize the comparison model using the shared feature extractor\n",
    "#         comparison_task = ComparisonModel(shared_feature_extractor)\n",
    "#         comparison_model = comparison_task.build_model()\n",
    "        \n",
    "#         # Define inputs for the joint model\n",
    "#         rating_input = layers.Input(shape=(192, 256, 3), name='rating_input')\n",
    "#         comparison_input_a = layers.Input(shape=(192, 256, 3), name='comparison_input_a')\n",
    "#         comparison_input_b = layers.Input(shape=(192, 256, 3), name='comparison_input_b')\n",
    "        \n",
    "#         # Define outputs for the joint model\n",
    "#         rating_output = rating_model(rating_input)\n",
    "#         comparison_output = comparison_model([comparison_input_a, comparison_input_b])\n",
    "        \n",
    "#         # Create the joint model\n",
    "#         joint_model = models.Model(\n",
    "#             inputs=[rating_input, comparison_input_a, comparison_input_b],\n",
    "#             outputs=[rating_output, comparison_output],\n",
    "#             name='joint_model'\n",
    "#         )\n",
    "        \n",
    "#         sgd = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9, nesterov=True)\n",
    "\n",
    "#         # Compile the joint model with the current weight combination\n",
    "#         joint_model.compile(\n",
    "#             optimizer=sgd, \n",
    "#             loss={\n",
    "#                 'rating_model': losses.MeanAbsoluteError(), \n",
    "#                 'comparison_model': 'bradley_terry_loss'\n",
    "#             },\n",
    "#             loss_weights={\n",
    "#                 'rating_model': rating_weight,\n",
    "#                 'comparison_model': comparison_weight\n",
    "#             },  \n",
    "#             metrics={\n",
    "#                 'rating_model': 'rating_rmse', \n",
    "#                 'comparison_model': 'custom_binary_accuracy'\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#         # Instantiate a new MetricsCallback for each training session\n",
    "#         metrics_callback = MetricsCallback(\n",
    "#             X_val=X_val,\n",
    "#             y_val=y_val,\n",
    "#             X_pairs_val=X_pairs_val,\n",
    "#             y_pairs_val=y_pairs_val,\n",
    "#             rating_model=rating_model,\n",
    "#             comparison_model=comparison_model,\n",
    "#             start_epoch=130\n",
    "#         )\n",
    "\n",
    "#         # Train the model\n",
    "#         history = joint_model.fit(\n",
    "#             train_generator,\n",
    "#             validation_data=val_generator,\n",
    "#             epochs=epochs,\n",
    "#             callbacks=[metrics_callback]\n",
    "#         )\n",
    "        \n",
    "#         # Store the best metrics for this combination\n",
    "#         result = {\n",
    "#             'rating_weight': rating_weight,\n",
    "#             'comparison_weight': comparison_weight,\n",
    "#             'best_pearson': metrics_callback.best_pearson,\n",
    "#             'best_pearson_epoch': metrics_callback.best_pearson_epoch,\n",
    "#             'best_accuracy': metrics_callback.best_accuracy,\n",
    "#             'best_accuracy_epoch': metrics_callback.best_accuracy_epoch,\n",
    "#             'epochs_with_thresholds': metrics_callback.epochs_with_thresholds,\n",
    "#             'pearson_values': metrics_callback.pearson_values,\n",
    "#             'accuracy_values': metrics_callback.accuracy_values\n",
    "#         }\n",
    "        \n",
    "#         results.append(result)\n",
    "\n",
    "#         # After training, print and log the summary for this weight combination\n",
    "#         summary_msg = f\"\\nSummary for Weight Combination - Rating: {rating_weight}, Comparison: {comparison_weight}\\n\"\n",
    "#         summary_msg += f\"Best Pearson Correlation: {result['best_pearson']:.4f} at epoch {result['best_pearson_epoch']}\\n\"\n",
    "#         summary_msg += f\"Best Accuracy: {result['best_accuracy']:.4f} at epoch {result['best_accuracy_epoch']}\\n\"\n",
    "        \n",
    "#         if result['epochs_with_thresholds']:\n",
    "#             summary_msg += f\"Epochs meeting thresholds:\\n\"\n",
    "#             for epoch in result['epochs_with_thresholds']:\n",
    "#                 pearson_value = result['pearson_values'][epoch - 1]\n",
    "#                 accuracy_value = result['accuracy_values'][epoch - 1]\n",
    "#                 summary_msg += f\"  Epoch {epoch}: Pearson Correlation = {pearson_value:.4f}, Accuracy = {accuracy_value:.4f}\\n\"\n",
    "#         else:\n",
    "#             summary_msg += 'No epochs met both thresholds.\\n'\n",
    "        \n",
    "#         print(summary_msg)\n",
    "#         f.write(summary_msg + \"\\n\")\n",
    "#         f.flush()  # Ensure the summary is written to the file immediately\n",
    "    \n",
    "#     # Optionally, log a final summary of all results\n",
    "#     final_summary_msg = \"\\nSummary of All Results:\\n\"\n",
    "#     for res in results:\n",
    "#         final_summary_msg += (\n",
    "#             f\"\\nWeight Combination - Rating: {res['rating_weight']}, Comparison: {res['comparison_weight']}\\n\"\n",
    "#             f\"Best Pearson Correlation: {res['best_pearson']:.4f} at epoch {res['best_pearson_epoch']}\\n\"\n",
    "#             f\"Best Accuracy: {res['best_accuracy']:.4f} at epoch {res['best_accuracy_epoch']}\\n\"\n",
    "#             f\"Epochs meeting thresholds: {res['epochs_with_thresholds']}\\n\" if res['epochs_with_thresholds'] else 'No epochs met both thresholds.\\n'\n",
    "#         )\n",
    "    \n",
    "#     print(final_summary_msg)\n",
    "#     f.write(final_summary_msg + \"\\n\")\n",
    "#     f.flush()  # Ensure the final summary is written to the file immediately\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some slightly different attempts for grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from itertools import product\n",
    "\n",
    "# # Define weight combinations for the rating_model and comparison_model\n",
    "# weight_combinations = [(0.6, 0.5)]\n",
    "\n",
    "# # Use a single log file for all weight combinations\n",
    "# output_log_file = 'output_log.txt'\n",
    "\n",
    "# for i, loss_weights in enumerate(weight_combinations):\n",
    "#     print(f\"Training model with loss weights {loss_weights}\")\n",
    "\n",
    "#     # Step 1: Create the shared feature extractor\n",
    "#     model_base = SharedFeatureExtractor()\n",
    "#     shared_feature_extractor = model_base.build_model()\n",
    "      \n",
    "#     # Step 2: Create the rating and comparison models\n",
    "#     rating_task = RatingModel(shared_feature_extractor)\n",
    "#     rating_model = rating_task.build_model()\n",
    "      \n",
    "#     comparison_task = ComparisonModel(shared_feature_extractor)\n",
    "#     comparison_model = comparison_task.build_model()\n",
    "      \n",
    "#     # Step 3: Define inputs for the joint model\n",
    "#     rating_input = layers.Input(shape=(192, 256, 3), name='rating_input')\n",
    "#     comparison_input_a = layers.Input(shape=(192, 256, 3), name='comparison_input_a')\n",
    "#     comparison_input_b = layers.Input(shape=(192, 256, 3), name='comparison_input_b')\n",
    "      \n",
    "#     # Step 4: Define outputs for the joint model\n",
    "#     rating_output = rating_model(rating_input)\n",
    "#     comparison_output = comparison_model([comparison_input_a, comparison_input_b])\n",
    "      \n",
    "#     # Step 5: Create the joint model\n",
    "#     joint_model = models.Model(\n",
    "#         inputs=[rating_input, comparison_input_a, comparison_input_b],\n",
    "#         outputs=[rating_output, comparison_output],\n",
    "#         name='joint_model'\n",
    "#     )\n",
    "\n",
    "#     sgd = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9, nesterov=True)\n",
    "\n",
    "#       # Compile the model with the current loss weights\n",
    "#     joint_model.compile(optimizer=sgd, \n",
    "#                     loss={'rating_model': losses.MeanAbsoluteError(), \n",
    "#                               'comparison_model': 'bradley_terry_loss'},\n",
    "#                     loss_weights={'rating_model': loss_weights[0], \n",
    "#                                       'comparison_model': loss_weights[1]},\n",
    "#                     metrics={'rating_model': 'rating_rmse', \n",
    "#                                  'comparison_model': 'custom_binary_accuracy'})\n",
    "\n",
    "#     # Add a header to distinguish each training run\n",
    "#     with open(output_log_file, 'a') as f:\n",
    "#         f.write(f\"\\n=== Training Run {i+1} with Weights Rating: {loss_weights[0]}, Comparison: {loss_weights[1]} ===\\n\")\n",
    "    \n",
    "#     # Initialize logging callback\n",
    "#     custom_logging_callback = CustomLoggingCallback(output_log_file, X_val, y_val, X_pairs_val, y_pairs_val, rating_model, comparison_model)\n",
    "\n",
    "#     # Train the model using the data generators\n",
    "#     history = joint_model.fit(\n",
    "#         train_generator,\n",
    "#         validation_data=val_generator,\n",
    "#         epochs=epochs,\n",
    "#         callbacks=[custom_logging_callback]\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Define weight combinations for the rating_model and comparison_model\n",
    "# weight_combinations = [(0.5, 0.5)]\n",
    "\n",
    "# # Store results for each weight combination\n",
    "# results = []\n",
    "\n",
    "# with open('output_log.txt', 'a') as f:\n",
    "\n",
    "#     # Loop through each weight combination\n",
    "#     for rating_weight, comparison_weight in weight_combinations:\n",
    "        \n",
    "#         # Print and log the starting message\n",
    "#         msg = f'\\nTraining with rating_model weight: {rating_weight}, comparison_model weight: {comparison_weight}'\n",
    "#         print(msg)\n",
    "#         f.write(msg + \"\\n\")\n",
    "#         f.flush()  # Ensure the message is written to the file immediately\n",
    "#         # print(f'\\nTraining with rating_model weight: {rating_weight}, comparison_model weight: {comparison_weight}')\n",
    "\n",
    "\n",
    "#         # Reinitialize the shared feature extractor\n",
    "#         model_base = SharedFeatureExtractor()\n",
    "#         shared_feature_extractor = model_base.build_model()\n",
    "        \n",
    "#         # Reinitialize the rating model using the shared feature extractor\n",
    "#         rating_task = RatingModel(shared_feature_extractor)\n",
    "#         rating_model = rating_task.build_model()\n",
    "        \n",
    "#         # Reinitialize the comparison model using the shared feature extractor\n",
    "#         comparison_task = ComparisonModel(shared_feature_extractor)\n",
    "#         comparison_model = comparison_task.build_model()\n",
    "        \n",
    "#         # Define inputs for the joint model\n",
    "#         rating_input = layers.Input(shape=(192, 256, 3), name='rating_input')\n",
    "#         comparison_input_a = layers.Input(shape=(192, 256, 3), name='comparison_input_a')\n",
    "#         comparison_input_b = layers.Input(shape=(192, 256, 3), name='comparison_input_b')\n",
    "        \n",
    "#         # Define outputs for the joint model\n",
    "#         rating_output = rating_model(rating_input)\n",
    "#         comparison_output = comparison_model([comparison_input_a, comparison_input_b])\n",
    "        \n",
    "#         # Create the joint model\n",
    "#         joint_model = models.Model(\n",
    "#             inputs=[rating_input, comparison_input_a, comparison_input_b],\n",
    "#             outputs=[rating_output, comparison_output],\n",
    "#             name='joint_model'\n",
    "#         )\n",
    "        \n",
    "#         sgd = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9, nesterov=True)\n",
    "\n",
    "#         # Compile the joint model with the current weight combination\n",
    "#         joint_model.compile(\n",
    "#             optimizer=sgd, \n",
    "#             loss={\n",
    "#                 'rating_model': losses.MeanAbsoluteError(), \n",
    "#                 'comparison_model': 'bradley_terry_loss'\n",
    "#             },\n",
    "#             loss_weights={\n",
    "#                 'rating_model': rating_weight,\n",
    "#                 'comparison_model': comparison_weight\n",
    "#             },  \n",
    "#             metrics={\n",
    "#                 'rating_model': 'rating_rmse', \n",
    "#                 'comparison_model': 'custom_binary_accuracy'\n",
    "#             }\n",
    "#         )\n",
    "\n",
    "#         # Instantiate a new MetricsCallback for each training session\n",
    "#         metrics_callback = MetricsCallback(\n",
    "#             X_val=X_val,\n",
    "#             y_val=y_val,\n",
    "#             X_pairs_val=X_pairs_val,\n",
    "#             y_pairs_val=y_pairs_val,\n",
    "#             rating_model=rating_model,\n",
    "#             comparison_model=comparison_model,\n",
    "#             start_epoch=130\n",
    "#         )\n",
    "\n",
    "#         # Train the model\n",
    "#         history = joint_model.fit(\n",
    "#             train_generator,\n",
    "#             validation_data=val_generator,\n",
    "#             epochs=epochs,\n",
    "#             callbacks=[metrics_callback]\n",
    "#         )\n",
    "        \n",
    "#         # Store the best metrics for this combination\n",
    "#         result = {\n",
    "#             'rating_weight': rating_weight,\n",
    "#             'comparison_weight': comparison_weight,\n",
    "#             'best_pearson': metrics_callback.best_pearson,\n",
    "#             'best_pearson_epoch': metrics_callback.best_pearson_epoch,\n",
    "#             'best_accuracy': metrics_callback.best_accuracy,\n",
    "#             'best_accuracy_epoch': metrics_callback.best_accuracy_epoch,\n",
    "#             'epochs_with_thresholds': metrics_callback.epochs_with_thresholds,\n",
    "#             'pearson_values': metrics_callback.pearson_values,\n",
    "#             'accuracy_values': metrics_callback.accuracy_values\n",
    "#         }\n",
    "        \n",
    "#         results.append(result)\n",
    "\n",
    "#         # After training, print and log the summary for this weight combination\n",
    "#         summary_msg = f\"\\nSummary for Weight Combination - Rating: {rating_weight}, Comparison: {comparison_weight}\\n\"\n",
    "#         summary_msg += f\"Best Pearson Correlation: {result['best_pearson']:.4f} at epoch {result['best_pearson_epoch']}\\n\"\n",
    "#         summary_msg += f\"Best Accuracy: {result['best_accuracy']:.4f} at epoch {result['best_accuracy_epoch']}\\n\"\n",
    "        \n",
    "#         if result['epochs_with_thresholds']:\n",
    "#             summary_msg += f\"Epochs meeting thresholds:\\n\"\n",
    "#             for epoch in result['epochs_with_thresholds']:\n",
    "#                 pearson_value = result['pearson_values'][epoch - 1]\n",
    "#                 accuracy_value = result['accuracy_values'][epoch - 1]\n",
    "#                 summary_msg += f\"  Epoch {epoch}: Pearson Correlation = {pearson_value:.4f}, Accuracy = {accuracy_value:.4f}\\n\"\n",
    "#         else:\n",
    "#             summary_msg += 'No epochs met both thresholds.\\n'\n",
    "        \n",
    "#         print(summary_msg)\n",
    "#         f.write(summary_msg + \"\\n\")\n",
    "#         f.flush()  # Ensure the summary is written to the file immediately\n",
    "    \n",
    "#     # Optionally, log a final summary of all results\n",
    "#     final_summary_msg = \"\\nSummary of All Results:\\n\"\n",
    "#     for res in results:\n",
    "#         final_summary_msg += (\n",
    "#             f\"\\nWeight Combination - Rating: {res['rating_weight']}, Comparison: {res['comparison_weight']}\\n\"\n",
    "#             f\"Best Pearson Correlation: {res['best_pearson']:.4f} at epoch {res['best_pearson_epoch']}\\n\"\n",
    "#             f\"Best Accuracy: {res['best_accuracy']:.4f} at epoch {res['best_accuracy_epoch']}\\n\"\n",
    "#             f\"Epochs meeting thresholds: {res['epochs_with_thresholds']}\\n\" if res['epochs_with_thresholds'] else 'No epochs met both thresholds.\\n'\n",
    "#         )\n",
    "    \n",
    "#     print(final_summary_msg)\n",
    "#     f.write(final_summary_msg + \"\\n\")\n",
    "#     f.flush()  # Ensure the final summary is written to the file immediately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dynamic weight callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "# class DynamicLossWeights(Callback):\n",
    "#     def __init__(self, initial_alpha, model):\n",
    "#         super(DynamicLossWeights, self).__init__()\n",
    "#         self.alpha = initial_alpha\n",
    "#         self.model = model\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         rating_loss = logs.get('rating_model_loss')\n",
    "#         comparison_loss = logs.get('comparison_model_loss')\n",
    "        \n",
    "#         # Adjust weights based on current losses\n",
    "#         if rating_loss is not None and comparison_loss is not None:\n",
    "#             total_loss = rating_loss + comparison_loss\n",
    "#             self.alpha = rating_loss / total_loss\n",
    "#             self.model.loss_weights = {\n",
    "#                 'rating_model': self.alpha,\n",
    "#                 'comparison_model': 1.0 - self.alpha\n",
    "#             }\n",
    "#         print(f\"Epoch {epoch+1} - Adjusted Weights: rating_model: {self.alpha}, comparison_model: {1.0 - self.alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ensemble accuracy calculation comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manually select specific IDs\n",
    "# manual_pair_ids = [24, 56, 62, 45, 16, 87]  # Replace with desired IDs\n",
    "\n",
    "# # Plotting\n",
    "# fig, axes = plt.subplots(len(manual_pair_ids), 2, figsize=(12, len(manual_pair_ids) * 4))\n",
    "# fig.subplots_adjust(hspace=0.4)\n",
    "# i = 0\n",
    "# for idx, id in enumerate(manual_pair_ids):\n",
    "#     img1, img2 = X_pairs_val[id]\n",
    "    \n",
    "#     # Plot first image of the pair\n",
    "#     axes[idx, 0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "#     axes[idx, 0].axis('off')\n",
    "#     axes[idx, 0].set_title('Image a', fontsize=12, pad=10)\n",
    "    \n",
    "#     # Plot second image of the pair\n",
    "#     axes[idx, 1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "#     axes[idx, 1].axis('off')\n",
    "#     axes[idx, 1].set_title('Image b', fontsize=12, pad=10)\n",
    "    \n",
    "#     # Add text with the true label and predicted value in the middle between the images\n",
    "#     fig.text(0.5, (len(manual_pair_ids) - idx - 0.5) / len(manual_pair_ids), \n",
    "#              f'True Label: {y_pairs_val[id]}\\nPredicted value: {comparison_predictions[id]:.2f}', \n",
    "#              ha='center', va='center', fontsize=14)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# binary_predictions = np.where(comparison_predictions >= 0.0, 1.0, -1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old rating model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OldLRN(Layer):\n",
    "\n",
    "    def __init__(self, n=5, alpha=0.0001, beta=0.75, k=2, **kwargs):\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.k = k\n",
    "        super(OldLRN, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.shape = input_shape\n",
    "        super(OldLRN, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            _, f, r, c = self.shape\n",
    "        else:\n",
    "            _, r, c, f = self.shape\n",
    "        half_n = self.n // 2\n",
    "        squared = tf.square(x)\n",
    "        pooled = tf.nn.pool(squared, window_shape=(half_n, half_n), pooling_type='AVG', strides=(1, 1),\n",
    "                         padding=\"SAME\")\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            summed = tf.reduce_sum(pooled, axis=1, keepdims=True)\n",
    "            averaged = (self.alpha / self.n) * tf.repeat(summed, f, axis=1)\n",
    "        else:\n",
    "            summed = tf.reduce_sum(pooled, axis=3, keepdims=True)\n",
    "            averaged = (self.alpha / self.n) * tf.repeat(summed, f, axis=3)\n",
    "        denom = tf.pow(self.k + averaged, self.beta)\n",
    "        return x / denom\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "input_shape = (192, 256, 3)\n",
    "l = 0.001 # weight decay\n",
    "\n",
    "\n",
    "old_im_data = layers.Input(shape=input_shape, dtype='float32', name='im_data')\n",
    "\n",
    "old_conv1 = layers.Conv2D(96, kernel_size=(11, 11), strides=(4, 4), name='conv1',\n",
    "                        activation='relu', input_shape=input_shape,\n",
    "                        kernel_regularizer=regularizers.l2(l))(old_im_data)\n",
    "\n",
    "old_pool1 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding=\"same\", name='pool1')(old_conv1)\n",
    "old_norm1 = OldLRN(name='norm1')(old_pool1)\n",
    "# norm1 = layers.BatchNormalization(name='norm1')(pool1)\n",
    "old_drop1 = layers.Dropout(0.1, name='dropout1')(old_norm1)\n",
    "\n",
    "old_layer1_1 = layers.Lambda(lambda x: x[:, :, :, :48], name='split1_1')(old_drop1)\n",
    "old_layer1_2 = layers.Lambda(lambda x: x[:, :, :, 48:], name='split1_2')(old_drop1)\n",
    "\n",
    "old_conv2_1 = layers.Conv2D(128, kernel_size=(5, 5), strides=(1, 1),\n",
    "                        activation='relu', padding='same',\n",
    "                        name='conv2_1', kernel_regularizer=regularizers.l2(l))(old_layer1_1)\n",
    "\n",
    "old_conv2_2 = layers.Conv2D(128, kernel_size=(5, 5), strides=(1, 1),\n",
    "                        activation='relu', padding='same',\n",
    "                        name='conv2_2', kernel_regularizer=regularizers.l2(l))(old_layer1_2)\n",
    "\n",
    "old_conv2 = layers.Concatenate(name='conv_2')([old_conv2_1, old_conv2_2])\n",
    "\n",
    "old_pool2 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool2')(old_conv2)\n",
    "old_norm2 = OldLRN(name=\"norm2\")(old_pool2)\n",
    "# norm2 = layers.BatchNormalization(name=\"norm2\")(pool2)\n",
    "old_drop2 = layers.Dropout(0.1, name='dropout2')(old_norm2)\n",
    "\n",
    "old_conv3 = layers.Conv2D(384, kernel_size=(3, 3), strides=(1, 1), activation='relu',\n",
    "                        name='conv3', padding='same',\n",
    "                        kernel_regularizer=regularizers.l2(l))(old_drop2)\n",
    "old_drop3 = layers.Dropout(0.1, name='dropout3')(old_conv3)\n",
    "\n",
    "old_layer3_1 = layers.Lambda(lambda x: x[:, :, :, :192], name='split3_1')(old_drop3)\n",
    "old_layer3_2 = layers.Lambda(lambda x: x[:, :, :, 192:], name='split3_2')(old_drop3)\n",
    "\n",
    "old_conv4_1 = layers.Conv2D(192, kernel_size=(3, 3), strides=(1, 1),\n",
    "                        activation='relu', padding='same',\n",
    "                        name='conv4_1', kernel_regularizer=regularizers.l2(l))(old_layer3_1)\n",
    "\n",
    "old_conv4_2 = layers.Conv2D(192, kernel_size=(3, 3), strides=(1, 1),\n",
    "                        activation='relu', padding='same',\n",
    "                        name='conv4_2', kernel_regularizer=regularizers.l2(l))(old_layer3_2)\n",
    "\n",
    "old_conv4 = layers.Concatenate(name='conv_4')([old_conv4_1, old_conv4_2])\n",
    "\n",
    "old_layer4_1 = layers.Lambda(lambda x: x[:, :, :, :192], name='split4_1')(old_conv4)\n",
    "old_layer4_2 = layers.Lambda(lambda x: x[:, :, :, 192:], name='split4_2')(old_conv4)\n",
    "\n",
    "old_conv5_1 = layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1),\n",
    "                        activation='relu', padding='same',\n",
    "                        name='conv5_1', kernel_regularizer=regularizers.l2(l))(old_layer4_1)\n",
    "\n",
    "old_conv5_2 = layers.Conv2D(128, kernel_size=(3, 3), strides=(1, 1),\n",
    "                        activation='relu', padding='same',\n",
    "                        name='conv5_2', kernel_regularizer=regularizers.l2(l))(old_layer4_2)\n",
    "\n",
    "old_conv5 = layers.Concatenate(name='conv_5')([old_conv5_1, old_conv5_2])\n",
    "\n",
    "old_pool5 = layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), name='pool5')(old_conv5)\n",
    "\n",
    "old_flat = layers.Flatten()(old_pool5)\n",
    "old_fc6 = layers.Dense(1024, activation='relu', name='Ratingfc6',\n",
    "                    kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "                    bias_initializer='zeros',\n",
    "                    kernel_regularizer=regularizers.l2(l))(old_flat)\n",
    "old_drop6 = layers.Dropout(0.5, name='RatingDropout6')(old_fc6) \n",
    "\n",
    "old_fc7 = layers.Dense(512, activation='relu', name='Ratingfc7',\n",
    "                    kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "                    bias_initializer='zeros',\n",
    "                    kernel_regularizer=regularizers.l2(l))(old_drop6)\n",
    "old_drop7 = layers.Dropout(0.5, name='RatingDropout7')(old_fc7)\n",
    "\n",
    "old_fc8 = layers.Dense(1, name='rating_output',\n",
    "                    kernel_initializer=initializers.RandomNormal(mean=0.0, stddev=0.01),\n",
    "                    bias_initializer='zeros')(old_drop7)\n",
    "\n",
    "old_rating_model = models.Model(inputs=old_im_data, outputs=old_fc8, name = 'old_rating_model')\n",
    "\n",
    "\n",
    "old_rating_model.get_layer('conv1').set_weights([conv1_kernel[:, :, :, :], conv1_bias[:]])\n",
    "old_rating_model.get_layer('conv2_1').set_weights([conv2_kernel[:, :, :, :128], conv2_bias[:128]])\n",
    "old_rating_model.get_layer('conv2_2').set_weights([conv2_kernel[:, :, :, 128:], conv2_bias[128:]])\n",
    "old_rating_model.get_layer('conv3').set_weights([conv3_kernel[:, :, :, :], conv3_bias[:]])\n",
    "old_rating_model.get_layer('conv4_1').set_weights([conv4_kernel[:, :, :, :192], conv4_bias[:192]])\n",
    "old_rating_model.get_layer('conv4_2').set_weights([conv4_kernel[:, :, :, 192:], conv4_bias[192:]])\n",
    "old_rating_model.get_layer('conv5_1').set_weights([conv5_kernel[:, :, :, :128], conv5_bias[:128]])\n",
    "old_rating_model.get_layer('conv5_2').set_weights([conv5_kernel[:, :, :, 128:], conv5_bias[128:]])\n",
    "\n",
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    return 0.5 * tf.reduce_mean(tf.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "    # Custom RMSE function for rating output\n",
    "def old_rating_rmse(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "class RatingDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, X, y, batch_size, normalize=True):\n",
    "        \"\"\"\n",
    "        Custom data generator class for CNN model.\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy.ndarray): Input data.\n",
    "        y (numpy.ndarray): Target labels.\n",
    "        batch_size (int): Size of each batch.\n",
    "        normalize (bool): Whether to normalize images to [0, 1]. Defaults to True.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.normalize = normalize\n",
    "        self.indexes = np.arange(len(self.X))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch.\"\"\"\n",
    "        return int(np.floor(len(self.X) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data.\"\"\"\n",
    "        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        X_batch = self.X[batch_indexes]\n",
    "        y_batch = self.y[batch_indexes]\n",
    "        \n",
    "        if self.normalize:\n",
    "            X_batch = self.normalize_images(X_batch)\n",
    "        \n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexes after each epoch.\"\"\"\n",
    "        np.random.shuffle(self.indexes)\n",
    "\n",
    "    def normalize_images(self, images):\n",
    "        \"\"\"Normalize images to [0, 1].\"\"\"\n",
    "        return images / 255.0\n",
    "    \n",
    "# Create data generators\n",
    "old_rating_gen_train = RatingDataGenerator(X_train, y_train, batch_size)\n",
    "old_rating_gen_val = RatingDataGenerator(X_val, y_val, batch_size)\n",
    "\n",
    "epochs = 95\n",
    "base_lr = 0.001\n",
    "old_rating_initial_learning_rate = base_lr\n",
    "old_rating_lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    old_rating_initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=old_rating_lr_schedule, momentum=0.9, nesterov=True)\n",
    "\n",
    "old_rating_model.compile(loss=losses.MeanAbsoluteError(), optimizer=sgd,\n",
    "                    metrics={\n",
    "    'rating_output' : [old_rating_rmse],\n",
    "     })\n",
    "\n",
    "# Test predictions before training\n",
    "print(\"Initial predictions before training:\")\n",
    "for img in X_val[:5]:\n",
    "    img = img.reshape(1, 192, 256, 3)\n",
    "    print(old_rating_model.predict(img))\n",
    "\n",
    "\n",
    "old_rating_history = old_rating_model.fit(old_rating_gen_train,\n",
    "    steps_per_epoch = ntrain // batch_size,\n",
    "    epochs = epochs,\n",
    "    validation_data = old_rating_gen_val,\n",
    "    validation_steps = nval // batch_size)\n",
    "\n",
    "# Test predictions after training\n",
    "print(\"Predictions after training:\")\n",
    "for img in X_val[:5]:\n",
    "    img = img.reshape(1, 192, 256, 3)\n",
    "    print(old_rating_model.predict(img))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "old_rating_predictions = []\n",
    "\n",
    "for img in X_val:\n",
    "  img = img.reshape(1, 192, 256, 3)\n",
    "  pred = old_rating_model.predict(img)\n",
    "  old_rating_predictions.append(float(pred))\n",
    "\n",
    "old_rating_predictions = np.array(old_rating_predictions)\n",
    "\n",
    "image_ids = [87, 45, 49, 94, 14, 83] # test image IDs sorted in descending order according to the website's aesthetics level\n",
    "\n",
    "fig = plt.figure(figsize=(12, 16))\n",
    "i = 1\n",
    "for id in image_ids:\n",
    "  if 'english' in test_images[id]:\n",
    "    path = images_path + '/english_resized/' + test_images[id].rsplit('/', 1)[1]\n",
    "  else:\n",
    "    path = images_path + '/foreign_resized/' + test_images[id].rsplit('/', 1)[1]\n",
    "\n",
    "  plt.subplot(len(image_ids)//2, 2, i)\n",
    "  img = mping.imread(path)\n",
    "  plt.title('User average rating: ' + str(np.round(y_val[id],2)) + '\\nPredicted rating: ' + str(np.round(old_rating_predictions[id],2)) + '\\n(' + chr(97+i-1) + ')', y=-0.25)\n",
    "  plt.axis('off')\n",
    "  plt.imshow(img)\n",
    "\n",
    "  i += 1\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "corr, p, lo, hi = pearsonr_ci(y_val, old_rating_predictions)\n",
    "print('Pearsons correlation: r=%.2f, p=%.2e, CI=[%.2f, %.2f]' % (corr, p, lo, hi))\n",
    "rmse_test = sqrt(mean_squared_error(y_val, old_rating_predictions))\n",
    "print('RMSE: %.3f' % rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how did the multi task and how did the old rating model fare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(12, 16))\n",
    "i = 1\n",
    "for id in image_ids:\n",
    "    if 'english' in test_images[id]:\n",
    "        path = images_path + '/english_resized/' + test_images[id].rsplit('/', 1)[1]\n",
    "    else:\n",
    "        path = images_path + '/foreign_resized/' + test_images[id].rsplit('/', 1)[1]\n",
    "\n",
    "    plt.subplot(len(image_ids)//2, 2, i)\n",
    "    img = mping.imread(path)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Add the title with the user rating and predictions from both models\n",
    "    plt.title(\n",
    "        f\"User average rating: {np.round(y_val[id], 2)}\\n\"\n",
    "        f\"New Model Prediction: {np.round(rating_predictions[id], 2)}\\n\"\n",
    "        f\"Old Model Prediction: {np.round(old_rating_predictions[id], 2)}\\n\"\n",
    "        f\"({chr(97+i-1)})\",\n",
    "        y=-0.25\n",
    "    )\n",
    "\n",
    "    i += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hard coded multi task and rating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "image_ids = [87, 45, 49, 94, 14, 83]  # Test image IDs sorted in descending order according to the website's aesthetics level\n",
    "rating_model_hardcoded = [5.32, 5.13, 4.7, 3.91, 3.68, 3.09]  # Hardcoded values for the Rating_model predicted rating\n",
    "\n",
    "fig = plt.figure(figsize=(12, 16))\n",
    "i = 1\n",
    "for idx, id in enumerate(image_ids):\n",
    "    if 'english' in test_images[id]:\n",
    "        path = images_path + '/english_resized/' + test_images[id].rsplit('/', 1)[1]\n",
    "    else:\n",
    "        path = images_path + '/foreign_resized/' + test_images[id].rsplit('/', 1)[1]\n",
    "\n",
    "    plt.subplot(len(image_ids) // 2, 2, i)\n",
    "    img = mpimg.imread(path)\n",
    "    plt.title('User average rating: ' + str(np.round(y_val[id], 2)) + \n",
    "              '\\nMulti-task predicted rating: ' + str(np.round(rating_predictions[id], 2)) + \n",
    "              '\\nRating_model predicted rating: ' + str(rating_model_hardcoded[idx]) + \n",
    "              '\\n(' + chr(97 + i - 1) + ')', y=-0.3)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lastenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
